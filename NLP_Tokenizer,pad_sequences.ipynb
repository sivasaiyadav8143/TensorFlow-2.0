{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Tokenizer,pad_sequences.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfl9QB9k2wVMSbfUYhClD3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sivasaiyadav8143/TensorFlow-2.x/blob/main/NLP_Tokenizer%2Cpad_sequences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6li7sSvoD8If"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB9MowJEGMBf"
      },
      "source": [
        "sentences = [\r\n",
        "    'I love my dog',\r\n",
        "    'I love my cat',\r\n",
        "    'You love my dog!',\r\n",
        "    'Do you think my dog is amazing?'\r\n",
        "]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYVT4b1ZF6Qc"
      },
      "source": [
        "token = Tokenizer(num_words=100)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjmdGrC1GHig",
        "outputId": "95e3f16b-dfca-4527-edc9-32784d5297aa"
      },
      "source": [
        "token.fit_on_texts(sentences)\r\n",
        "tok_ind = token.index_word\r\n",
        "print(tok_ind)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 'my', 2: 'love', 3: 'dog', 4: 'i', 5: 'you', 6: 'cat', 7: 'do', 8: 'think', 9: 'is', 10: 'amazing'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkgbRCGMGZWp"
      },
      "source": [
        "text_seq = token.texts_to_sequences(sentences)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPH_GDMBGh2_",
        "outputId": "c647d50d-7364-48ed-8aa2-4e63c03eae81"
      },
      "source": [
        "print(text_seq)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kw4XQ7neJHPo",
        "outputId": "30f16924-c3f2-420d-8cdc-5423a533514e"
      },
      "source": [
        "# padd = pad_sequences(text_seq,maxlen=5)\r\n",
        "\r\n",
        "padd = pad_sequences(text_seq)\r\n",
        "print(padd)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0  4  2  1  3]\n",
            " [ 0  0  0  4  2  1  6]\n",
            " [ 0  0  0  5  2  1  3]\n",
            " [ 7  5  8  1  3  9 10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pt2MbCSfGjxD",
        "outputId": "4a2af696-5c5c-4710-8e12-8840d28ead5c"
      },
      "source": [
        "# Try with words that the tokenizer wasn't fit to\r\n",
        "test_data = [\r\n",
        "    'i really love my dog',\r\n",
        "    'my dog loves my manatee'\r\n",
        "]\r\n",
        "\r\n",
        "text_seq1 = token.texts_to_sequences(test_data)\r\n",
        "print(text_seq1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4, 2, 1, 3], [1, 3, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OROKjclWHkV2"
      },
      "source": [
        "In the above line, some words are missing which were not seen during training like 'really', 'loves' and 'manatee'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eysnsTkKHfJG"
      },
      "source": [
        "sol_token = Tokenizer(num_words=100,oov_token='OOV')\r\n",
        "sol_token.fit_on_texts(sentences)\r\n",
        "sol_tok_ind = sol_token.index_word"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK2qfTcCIFP2",
        "outputId": "911b7ef7-9da3-4de8-bf6c-a6ad734d207b"
      },
      "source": [
        "print(sol_tok_ind)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 'OOV', 2: 'my', 3: 'love', 4: 'dog', 5: 'i', 6: 'you', 7: 'cat', 8: 'do', 9: 'think', 10: 'is', 11: 'amazing'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPq62ITrIXlB"
      },
      "source": [
        "sol_text_seq = sol_token.texts_to_sequences(sentences)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4obk_LBkItHJ",
        "outputId": "51a4c322-e4da-4741-da2c-67b715ee17af"
      },
      "source": [
        "print(sol_text_seq)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BrnC_AGIvHC",
        "outputId": "3bc5a14f-1d04-4a92-97d7-d643ae21914c"
      },
      "source": [
        "# Try with words that the tokenizer wasn't fit to\r\n",
        "\r\n",
        "sol_text_seq1 = sol_token.texts_to_sequences(test_data)\r\n",
        "print(sol_text_seq1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkfMME95I7nZ",
        "outputId": "c0b34322-77a1-4498-f23d-2ccd8d3a7d45"
      },
      "source": [
        "sol_padd = pad_sequences(sol_text_seq1,maxlen=10)\r\n",
        "print(sol_padd)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 0 5 1 3 2 4]\n",
            " [0 0 0 0 0 2 4 1 2 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF4BdiDaJ7GI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}